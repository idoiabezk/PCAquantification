---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: console
---



```{r startup}
library(nnls)
library(tidyverse)
library(tidymodels)
library(readxl)

```



## Setup automatic nnls deconvolution process from excel sheet
```{r}
# reading data from excel
Skyline_output <- read_excel("./data/OrbitrapDust.xlsx") |>
  mutate(`Analyte Concentration` = as.numeric(`Analyte Concentration`)) |> 
  mutate(`Normalized Area` = as.numeric(`Normalized Area`)) |> 
  mutate(`Normalized Area` = replace_na(`Normalized Area`, 0)) |> # Replace missing values in the Response_factor column with 0
  mutate(Area = replace_na(Area, 0)) |>
  mutate(RatioQuanToQual = as.numeric(RatioQuanToQual)) |> #--< convert to numeric #-->
  mutate(RatioQualToQuan = as.numeric(RatioQualToQuan)) #--< convert to numeric #-->



CPs_standards <- Skyline_output |> 
  filter(`Sample Type` == "Standard",
         Molecule != "IS",
         Molecule != "RS",
         `Isotope Label Type` == "Quan",
         Note != "NA") |> 
  group_by(Note, Molecule) |>
  #mutate(`Normalized Area` = mean(`Normalized Area`)) |> 
  mutate(rel_int = `Normalized Area`/sum(`Normalized Area`)) |> 
  nest() |> 
  mutate(models = map(data, ~lm(`Normalized Area` ~ `Analyte Concentration`, data = .x))) |> 
  mutate(coef = map(models, coef)) |> 
  mutate(slope = map(coef, pluck("`Analyte Concentration`"))) |> 
  mutate(intercept = map(coef, pluck("(Intercept)"))) |> 
  mutate(rsquared = map(models, summary)) |> 
  mutate(rsquared = map(rsquared, pluck("r.squared"))) 

# NEED TO ADD using case_when or elseif..
# if (sum(!is.na(filtered_dataA$`Normalized Area`)) == 0 || sum(!is.na(filtered_dataA$`Analyte Concentration`)) == 0) {
#       cat("No valid cases for fitting the model for molecule:", molecule_name, "\n")
# OR add a filter to remove those rsquared that are very low (perhaps below 0.7?)  

# unnest() |> 
# ungroup() |> 
#   select(Molecule, rel_int) |> 
#   column_to_rownames(var = "Molecule") |> 
#   t() |> 
#   as.vector()


```



----TEST SCRIPTS-----


## test using Bogdal paper Figure 1
```{r}

s <- c(0.1, 0.4, 0.4, 0.1)
y1 <- c(0.1, 0.4, 0.3, 0.2)
y2 <- c(0.5, 0.1, 0.2, 0.2)
y3 <- c(0.2, 0.4, 0.4, 0)
Y <- cbind(y1, y2, y3)
deconv <- nnls(Y, s)
deconv
coefficients <- deconv$x
resolved <- c(y1*deconv$x[1]+y2*deconv$x[2]+y3*deconv$x[3])
resolved2 <- deconv[["fitted"]]

chisq.test(resolved, p = s, rescale.p = TRUE)
qchisq(.5, df = 3)


reconstructed_pattern <- rowSums(Y%*%coefficients)




```


## Test using data from Gao et al std 57, 61, 63 and sample 74

```{r}
y1 <- c(0,0,0.00890544,0.021319085,0.019430052,0.023855786,0,0.003022453,0.064173143,0.143404577,0.087543178,0.028875216,0,0.003562176,0.048197323,0.146265112,0.13800734,0.04015544,0,0.007016408,0.022398532,0.074805699,0.096556563,0.022506477)
y2<- c(0,	0.000420628,	0.014890216,0.020947253,0.019432994,0.016488601,0,0.015394969,0.091192059,0.125094641,0.070665433,0.022798015,0,0.014385463,0.083789013,0.143097501,0.103053756,0.028182048,0.001009506,0.012450576,0.041389754,0.081854126,0.07596534,0.017498107)
y3 <- c(0,0,0.009719222,0.020347846,0.018983744,0.024155962,0,0.002671365,0.05632602,0.137944754,0.08821189,0.029726043,0,0.001648289,0.039047403,0.140729794,0.139365693,0.045413209,0.007332045,0,0.017051267,0.071331136,0.106172559,0.043821757)
A <- as.matrix(cbind(y1,y2, y3))

s <- c(0,0.002484031,0.038549451,0.128585146,0.133542771,0.066473928,0,0.00241619,0.024986432,0.101563478,0.147058824,0.072579635,0.000741035,0.005437732,0.01244103,0.027548741,0.041403999,0.023180812,0.003846074,0.011256419,0.031332192,0.043131341,0.047916754,0.033523984)

a <- nnls(A,s)
a
coefficients <- a$x
resolved <- c(y1*a[["x"]][1] + y2*a[["x"]][2] + y3*a[["x"]][3])
chisq.test(resolved, p = s, rescale.p = TRUE)

sresolved <- cbind(s, resolved)
chisq.test(sresolved)

```


## test tidymodels 
```{r}
library(RcppML)
library(Matrix)

data(biomass, package = "modeldata")

rec <- recipe(HHV ~ ., data = biomass) %>%
  update_role(sample, new_role = "id var") %>%
  update_role(dataset, new_role = "split variable") %>%
  step_nnmf_sparse(
    all_numeric_predictors(),
    num_comp = 2,
    seed = 473,
    penalty = 0.01
  ) %>%
  prep(training = biomass)

bake(rec, new_data = NULL)


bake(rec, new_data = NULL) %>%
  ggplot(aes(x = NNMF2, y = NNMF1, col = HHV)) +
  geom_point()


```


glmnet
```{r}

library(glmnet)
mod2 <- glmnet(x, y, lambda = 0, lower.limits = 0, intercept = FALSE)
coef(mod2)

```


Neural network
```{r}

library(tidymodels)
library(brulee)

# Example data (replace with your own)
# Create synthetic data for illustration
set.seed(123)
n_obs <- 100
n_features <- 5

# Generate random features
features <- matrix(rnorm(n_obs * n_features), nrow = n_obs)

# Create response variable (classification example)
response <- sample(c("A", "B", "C"), size = n_obs, replace = TRUE)

# Combine features and response into data frames
cls_train <- data.frame(features, class = response)
cls_val <- data.frame(features, class = sample(c("A", "B", "C"), size = n_obs, replace = TRUE))

# Check the structure of cls_train and cls_val
str(cls_train)
str(cls_val)

# Create a recipe
biv_rec <- recipe(class ~ ., data = cls_train) %>%
  step_normalize(all_predictors())

# Specify the neural network model
nnet_spec <- mlp(epochs = 1000, hidden_units = 10, penalty = 0.01, learn_rate = 0.1) %>%
  set_engine("brulee", validation = 0) %>%
  set_mode("classification")

# Create a workflow
nnet_wflow <- biv_rec %>% workflow(nnet_spec)

# Fit the model
set.seed(987)
nnet_fit <- fit(nnet_wflow, cls_train)


```




```{r}
# Install and load the nnls package (if not already installed)
# install.packages("nnls")
library(nnls)

# Example vectors
a <- c(1, 2, 3)
b <- c(3, 2, 1)
target_vector <- c(4, 5, 6)  # The vector you want to express

# Create a matrix from vectors a and b
ab_matrix <- matrix(c(a, b), nrow = length(a))

# Solve for coefficients
coefficients <- nnls(A = ab_matrix, b = target_vector)$x

# Print the coefficients
print(coefficients)

reconstr <- rowSums(coefficients*ab_matrix)



```


